{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import textblob   \n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import numpy as np     \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}\n",
    "\n",
    "Base_url = \"https://uk.reuters.com\"\n",
    "\n",
    "#from reuters oil archive\n",
    "pages = [str(i) for i in range(1,2000)]\n",
    "url_list=[]\n",
    "\n",
    "for page in pages:\n",
    "\n",
    "    url= 'https://uk.reuters.com/news/archive/oilRpt?view=page&page='+page +'&pageSize=10'\n",
    "    url_list.append(url)\n",
    "\n",
    "print(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import clear_output\n",
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "requests = 0\n",
    "\n",
    "for _ in range(5):\n",
    "    \n",
    "    requests += 1\n",
    "    sleep(randint(1,3))\n",
    "    current_time = time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    print('Request: {}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "    clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create an empty list which will contain the selected news articles \n",
    "List_of_links = [] \n",
    "word1=\"article\"\n",
    "\n",
    "for newsurl in url_list:\n",
    "    response = get(newsurl, headers = headers)\n",
    "    \n",
    "    sleep(randint(1,3))\n",
    "    \n",
    "    requests += 1\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Request:{}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # Throw a warning for non-200 status codes\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "    \n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    #page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    sub_links = html_soup.find_all('a')\n",
    "    for links in sub_links:\n",
    "        sp = BeautifulSoup(str(links),'html.parser')  # first convert into a string\n",
    "        tag = sp.a\n",
    "      #if word1 in tag['title'] or word2 in tag['title'] or word3 in tag['title']:\n",
    "        if word1 in tag['href']:\n",
    "            category_links =  Base_url + tag[\"href\"]\n",
    "            List_of_links.append(category_links)\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unique_links = list(set(List_of_links))\n",
    "for q in unique_links: print(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "                   'url_news'  : unique_links,\n",
    "                  })\n",
    "\n",
    "print(df.info())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#read in links\n",
    "\n",
    "dflinks=pd.read_csv('C:/Users/neil.watt/Documents/PythonScripts/Sentiment/Main/MarketSentiment_Analysis/links.csv')\n",
    "dflinks['url_news']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "unique_links=dflinks['url_news']\n",
    "# Create an empty list which will contain the selected news articles \n",
    "text_body=[]\n",
    "date=[]\n",
    "\n",
    "\n",
    "for selected_links in unique_links:\n",
    "    response = get(selected_links, headers = headers)\n",
    "    sleep(randint(1,3))\n",
    "    requests += 1\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Request:{}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "    clear_output(wait = True)\n",
    "      # Throw a warning for non-200 status codes\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "        \n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #retrieve body of article\n",
    "    results_text = html_soup.find(class_='StandardArticleBody_body')\n",
    "    story_text = results_text.get_text()\n",
    "    lower_story_text=story_text.lower()\n",
    "    text_body.append(lower_story_text) \n",
    "    #retrieve date\n",
    "    date_results_text = html_soup.find(class_='ArticleHeader_date')\n",
    "    date_text = date_results_text.get_text()\n",
    "    date.append(date_text) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_articles.to_csv('df_articles.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
